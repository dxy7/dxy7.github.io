
<!DOCTYPE html>
<html lang="zh_cn" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>《Spark local&amp; stand-alone配置》 - DXY</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="《Spark local&amp;amp; stand-alone配置》
title: 《Spark local&amp;amp; stand-alone配置》date: 2022-05-22 16:25:01de,"> 
    <meta name="author" content="垂耳兔"> 
    <link rel="alternative" href="atom.xml" title="DXY" type="application/atom+xml"> 
    <link rel="icon" href="/img/13.jpg"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="《Spark local&amp; stand-alone配置》 - DXY"/>
    <meta name="twitter:description" content="《Spark local&amp;amp; stand-alone配置》
title: 《Spark local&amp;amp; stand-alone配置》date: 2022-05-22 16:25:01de,"/>
    
    
    
    
    <meta property="og:site_name" content="DXY"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="《Spark local&amp; stand-alone配置》 - DXY"/>
    <meta property="og:description" content="《Spark local&amp;amp; stand-alone配置》
title: 《Spark local&amp;amp; stand-alone配置》date: 2022-05-22 16:25:01de,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

    <script>window.searchDbPath = "/search.xml";</script>
<meta name="generator" content="Hexo 6.2.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">DXY</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">《Spark local& stand-alone配置》</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">《Spark local& stand-alone配置》</h1>
        <div class="stuff">
            <span>五月 22, 2022</span>
            

        </div>
        <div class="content markdown">
            <h1 id="《Spark-local-amp-stand-alone配置》"><a href="#《Spark-local-amp-stand-alone配置》" class="headerlink" title="《Spark local&amp; stand-alone配置》"></a>《Spark local&amp; stand-alone配置》</h1><hr>
<p>title: 《Spark local&amp; stand-alone配置》<br>date: 2022-05-22 16:25:01<br>description: Spark（local）   Spark（stand-alone）</p>
<details>
<summary>阅读全文</summary>

<p>**<summary>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境<br>Anaconda On Linux 安装 (单台服务器脚本安装)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装<br>位置在 &#x2F;export&#x2F;server:</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server<br>   #运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh<br>   过程显示：<br>   …<br>   #出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] </p>
<blockquote>
</blockquote>
<p>   &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p>
</blockquote>
</blockquote>
<p>安装完成后, 退出终端， 重新进来:</p>
<blockquote>
<blockquote>
<blockquote>
<p>exit<br>   结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>   [root@node1 ~]#</p>
</blockquote>
</blockquote>
</blockquote>
<p>创建虚拟环境 pyspark 基于 python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>切换到虚拟环境内</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p>
</blockquote>
</blockquote>
<p>在虚拟环境内安装包 （有WARNING不用管）</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
</blockquote>
</blockquote>
<p>spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;</p>
</blockquote>
</blockquote>
<p>建立软连接</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>添加环境变量</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   SPARK_HOME: 表示Spark安装路径在哪里<br>   PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>   JAVA_HOME: 告知Spark Java在哪里<br>   HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>   HADOOP_HOME: 告知Spark Hadoop安装在哪里</p>
</blockquote>
</blockquote>
<p>   vim &#x2F;etc&#x2F;profile<br>   内容：<br>   …..<br>   注：此部分之前配置过，此部分不需要在配置<br>   #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </p>
<p>   #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin </p>
<p>   #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin ….. </p>
<p>   #将以下部分添加进去 #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python<br>   vim .bashrc<br>   内容添加进去： </p>
<p>   #JAVA_HOME<br>   export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241<br>   #PYSPARK_PYTHON<br>   export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python</p>
<p>重新加载环境变量文件</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile<br>   source ~&#x2F;.bashrc</p>
</blockquote>
</blockquote>
<p>进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;</p>
</blockquote>
</blockquote>
<p>开启 </p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  .&#x2F;pyspark 结果显示： (base) [root@node1 bin]# .&#x2F;pyspark<br>   Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>   Type “help”, “copyright”, “credits” or “license” for more information.<br>   Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable<br>    Welcome to<br>         __              __<br>    __ &#x2F; <strong>&#x2F;</strong> ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>     <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F; ‘</em>&#x2F;<br>     &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\ version 3.2.0<br>        &#x2F;</em>&#x2F;<br>    Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a target="_blank" rel="noopener" href="http://node1:4040/">http://node1:4040</a> Spark context available as ‘sc’ (node1 &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt;</p>
</blockquote>
</blockquote>
<p>查看WebUI界面</p>
<blockquote>
<blockquote>
<blockquote>
<p>浏览器访问：<br>    <a target="_blank" rel="noopener" href="http://node1:4040/">http://node1:4040/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>退出</p>
<blockquote>
<blockquote>
<blockquote>
<p>conda deactivate</p>
</blockquote>
</blockquote>
</blockquote>
<p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 node2 和 node3 上部署)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p>
<p>cd &#x2F;export&#x2F;server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh</p>
<blockquote>
<blockquote>
<blockquote>
<p>过程显示：<br> …<br> #出现内容选 yes<br>  Please answer ‘yes’ or ‘no’:’<br>yes<br>   …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p>
</blockquote>
</blockquote>
</blockquote>
<p>安装完成后, 退出终端，</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  重新进来:<br>  exit<br>  结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>    Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>   …</p>
</blockquote>
</blockquote>
<p>在 node1 节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #分发 .bashrc : scp <del>&#x2F;.bashrc root@node2:</del>&#x2F; scp <del>&#x2F;.bashrc root@node3:</del>&#x2F; #分发 profile : scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F;<br>   …</p>
</blockquote>
</blockquote>
<p>创建虚拟环境 pyspark 基于 python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>切换到虚拟环境内</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>在虚拟环境内安装包 （有WARNING不用管）<br>    pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>    spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p>
</blockquote>
</blockquote>
<p>node1 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</p>
</blockquote>
</blockquote>
<p>将文件 workers.template 改名为 workers，并配置文件内容</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv workers.template workers vim workers<br>    # localhost删除，内容追加文末： node1<br>    node2<br>    node3<br>    # 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</p>
</blockquote>
</blockquote>
<p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-env.sh.template spark-env.sh vim spark-env.sh</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>文末追加内容：<br>   ##设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<br>   ##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大node1的IP和提交任务的通信端口 # 告知Spark的node1运行在哪个机器上 export SPARK_node1_HOST&#x3D;node1<br>    #告知sparknode1的通讯端口 export SPARK_node1_PORT&#x3D;7077<br>    # 告知spark node1的 webui端口 SPARK_node1_WEBUI_PORT&#x3D;8080<br>    # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1<br>    # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g<br>    # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078<br>    # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081<br>    ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;”- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;true”</p>
</blockquote>
</blockquote>
</blockquote>
<p>开启 hadoop 的 hdfs 和 yarn 集群</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>start-dfs.sh 
start-yarn.sh
</code></pre>
<p>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p>
<blockquote>
</blockquote>
<p>   hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog</p>
</blockquote>
</blockquote>
<p>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true</p>
</blockquote>
</blockquote>
<p>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为<br>log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：<br>    …<br>    18 # Set everything to be logged to the console<br>    19 log4j.rootCategory&#x3D;WARN, console ….</p>
</blockquote>
</blockquote>
<p>node1 节点分发 spark 安装文件夹 到 node2 和 node3 上</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   node1 节点分发 spark 安装文件夹 到 node2 和 node3 上</p>
</blockquote>
</blockquote>
<p>在node2 和 node3 上做软连接</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>重新加载环境变量</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh</p>
</blockquote>
</blockquote>
<p>访问 WebUI 界面</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   浏览器访问： <a target="_blank" rel="noopener" href="http://node1:18080/">http://node1:18080/</a></p>
</blockquote>
</blockquote>
<p>readme.md<br>**</p>
</details>

<hr>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="true">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title="0" data-url="http://link.hhtjim.com/163/34167858.mp3"></li>
                        
                    
                </ul>
            
        </div>
        
    <div id="gitalk-container" class="comment link"
		data-enable="true"
        data-ae="false"
        data-ci=""
        data-cs=""
        data-r=""
        data-o=""
        data-a=""
        data-d="false"
    >查看评论</div>


    </div>
    
        <div class="side">
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ASpark-local-amp-stand-alone%E9%85%8D%E7%BD%AE%E3%80%8B"><span class="toc-number">1.</span> <span class="toc-text">《Spark local&amp; stand-alone配置》</span></a></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>






</html>
